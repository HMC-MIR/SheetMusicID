{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import numba as nb\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import dill\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases_v2/probabilities.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 6), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_DB(db, combination, outdir):\n",
    "    with open(f\"{outdir}/{combination}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(db, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test database with in memory dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store each n-gram, utility, and the number of matches for an n-gram in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(combinations):\n",
    "    mapping[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pdfs = 30275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays(fp_matches):\n",
    "    n_grams = np.array([], dtype = object)\n",
    "    probabilities = np.array([], dtype = float)\n",
    "    matches = np.array([], dtype = int)\n",
    "    types = np.array([], dtype = int)\n",
    "    for combination in fp_matches:\n",
    "        d = fp_matches[combination]\n",
    "        length = len(d)       \n",
    "        # update indices for the given combination\n",
    "        fps = np.empty(length, dtype = object)\n",
    "        utils = np.empty(length, dtype = float)\n",
    "        values = np.empty(length, dtype = int)\n",
    "        cur_types = np.full(length, mapping[combination])\n",
    "        for i, (n_gram, (count, num_pdfs)) in enumerate(d.items()):\n",
    "            fps[i] = n_gram\n",
    "            values[i] = count\n",
    "            utils[i] = utilities[combination] * np.log2(num_pdfs / total_pdfs)\n",
    "        d.clear()\n",
    "        n_grams = np.concatenate([n_grams, fps])\n",
    "        probabilities = np.concatenate([probabilities, utils])\n",
    "        matches = np.concatenate([matches, values])\n",
    "        types = np.concatenate([types, cur_types])\n",
    "        print(f\"finished processing {combination}\")\n",
    "    return n_grams, probabilities, matches, types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark indices as boundaries for each type of n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_matches_file = f\"{db_dir}/fp_matches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fp_matches_file, \"rb\") as f:\n",
    "    fp_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing 0\n",
      "finished processing 01\n",
      "finished processing 02\n",
      "finished processing 03\n",
      "finished processing 04\n",
      "finished processing 05\n",
      "finished processing 012\n",
      "finished processing 013\n",
      "finished processing 014\n",
      "finished processing 015\n",
      "finished processing 023\n",
      "finished processing 024\n",
      "finished processing 025\n",
      "finished processing 034\n",
      "finished processing 035\n",
      "finished processing 045\n"
     ]
    }
   ],
   "source": [
    "n_grams, probs, matches, types = generate_arrays(fp_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(n_grams, \"fps\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(probs, \"utils\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(matches, \"matches\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(types, \"n_gram_types\", db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate database construction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def parallel_ratio(matches, utils):\n",
    "    z=np.empty(matches.shape)\n",
    "    for i in nb.prange(len(matches)):\n",
    "        z[i] = utils[i] / matches[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 6), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(n_grams, utilities, types, outdir, outfile_name):\n",
    "    \"\"\"\n",
    "    Input: an array of n-grams, an array of each n-gram's utility, an array of the number of matches\n",
    "           for each n-gram in IMSLP, and an array of each n-gram's type (e.g. '012')\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    # sort fingerprints in descending order\n",
    "    idx = np.argsort(-utilities)\n",
    "            \n",
    "    # write all used fingerprints to a file\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as out:\n",
    "        for i in idx:\n",
    "            out.write(f\"{n_grams[i]} {types[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{db_dir}/fps.pkl\", \"rb\") as f:\n",
    "    n_grams = pickle.load(f)\n",
    "with open(f\"{db_dir}/utils.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)\n",
    "with open(f\"{db_dir}/n_gram_types.pkl\", \"rb\") as f:\n",
    "    types = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_db(n_grams, utilities, types, \"/data1/kji/construction_lists\", \"all_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct database of offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected all the fingerprints, we construct the database containing each fingerprint and their offsets in IMSLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_file = \"/data1/kji/construction_lists/75mill_v3.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a dictionary with all the n-grams in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(combinations):\n",
    "    reverse_mapping[str(i)] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_entry(line):\n",
    "    line = line.rstrip().split()\n",
    "    n_gram, combination = ''.join(line[:-1]), line[-1]\n",
    "    return ast.literal_eval(n_gram), reverse_mapping[combination]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db(fp_file):\n",
    "    with open(fp_file) as f:\n",
    "        lines = f.readlines()\n",
    "    n_cores = 30\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    keys = pool.map(initialize_entry, lines)\n",
    "    dbs = {combination: {} for combination in combinations}\n",
    "    for fp, combination in keys:\n",
    "        dbs[combination][fp] = {}\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs = make_db(fp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in every single database file and update our current database with the real offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_db_values(file, db):\n",
    "    with open(file, \"rb\") as f:\n",
    "        d = dill.load(f)\n",
    "    for n_gram in d.keys():\n",
    "        if n_gram in db:\n",
    "            if db[n_gram] is None:\n",
    "                db[n_gram] = d[n_gram]\n",
    "            else:\n",
    "                db[n_gram].update(d[n_gram])\n",
    "    d.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in combinations:\n",
    "    if dbs[combination]:\n",
    "        add_db_values(f\"{db_dir}/{combination}.pkl\", dbs[combination])\n",
    "        d = dbs[combination]\n",
    "        for fp in d:\n",
    "            total_count = sum([len(d[fp][piece]) for piece in d[fp]])\n",
    "            dbs[combination][fp] = (total_count, d[fp])\n",
    "        with open(f\"/data1/kji/databases_v3/75mill/{combination}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(dbs[combination], f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        dbs[combination].clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('sqlite:////data1/kji/databases/test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['/data1/kji/databases/0_counts.pkl', \n",
    "              '/data1/kji/databases/01_counts.pkl',\n",
    "              '/data1/kji/databases/012_counts.pkl',\n",
    "              '/data1/kji/databases/0123_counts.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing /data1/kji/databases/0_counts.pkl\n",
      "finished processing /data1/kji/databases/01_counts.pkl\n",
      "finished processing /data1/kji/databases/012_counts.pkl\n",
      "finished processing /data1/kji/databases/0123_counts.pkl\n"
     ]
    }
   ],
   "source": [
    "for filename in test_files:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        df_dict = defaultdict(list)\n",
    "        combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "        for n_gram, count in d.items():\n",
    "            df_dict['combination'].append(combination)\n",
    "            df_dict['n_gram'].append(str(n_gram))\n",
    "            df_dict['matches'].append(count)\n",
    "            df_dict['utility'].append(utilities[combination])\n",
    "            df_dict['used'].append(0)\n",
    "        d.clear()\n",
    "        df = pd.DataFrame.from_dict(df_dict)\n",
    "        with engine.begin() as connection:\n",
    "            df.to_sql(combination, con=connection, index=False, if_exists='replace')\n",
    "        print(f\"finished processing {filename}\")\n",
    "        f.flush()\n",
    "        df_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate database construction plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_memory(num_matches, remaining_budget):\n",
    "    return num_matches / remaining_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget):\n",
    "    cost = (matches_squared + num_matches ** 2) / (total_matches + num_matches) - total_avg_runtime\n",
    "    return cost / (runtime_budget - total_avg_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(utility, num_matches, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    marginal_memory_cost = marginal_memory(num_matches, memory_budget-total_matches)\n",
    "    marginal_runtime_cost = marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget)\n",
    "    return utility / max(marginal_memory_cost, marginal_runtime_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/data1/kji/databases/test.db\")\n",
    "conn.create_function(\"metric\", 7, metric)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_items = ['(2048, 0, 0, 0, 0, 0)', '(1073741824, 0, 0, 0, 0, 0)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from fingerprints where used == 0 order by utility desc limit 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = c.execute(query).fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db_path, table_name, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    with closing(sqlite3.connect(db_path)) as con, con,  \\\n",
    "            closing(con.cursor()) as cur:\n",
    "        con.create_function(\"metric\", 7, metric)\n",
    "        query = f\"select * from '{table_name}' where used = 0 order by metric(utility, matches, ?, ?, ?, ?, ?) desc limit 1\"\n",
    "        cur.execute(query, (memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime))\n",
    "        result = cur.fetchone()\n",
    "        ratio = metric(result[3], result[2], memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime)\n",
    "        return (result, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(db_path, memory_budget, runtime_budget, outdir):\n",
    "    \"\"\"\n",
    "    Inputs: a database which contains n-grams, their utility, and number of matches,\n",
    "            a memory budget, and a runtime budget\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    with closing(conn.cursor()) as c:\n",
    "        c.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = c.fetchall()\n",
    "        \n",
    "    # clear all used bits in the database\n",
    "    for table in tables:\n",
    "        with closing(conn.cursor()) as cur:\n",
    "            cur.execute(f\"UPDATE '{table[0]}' SET used = 0\")\n",
    "    \n",
    "    total_matches = 0\n",
    "    matches_squared = 0\n",
    "    total_avg_runtime = 0\n",
    "    with open(f\"{outdir}/Bm_10k_Bt_8k_fingerprints.txt\", \"w\") as f1, open(f\"{outdir}/Bm_10k_Bt_8k_info.csv\", \"w\") as f2:\n",
    "        writer = csv.writer(f2)\n",
    "        writer.writerow([\"fingerprint\", \"ratio\", \"m_i\", \"Ct_i\", \"Bm-Dm\", \"Bt-Dt\"])\n",
    "        while memory_budget - total_matches > 0 and runtime_budget - total_avg_runtime > 0:\n",
    "            start = time.time()\n",
    "            inputs = [(db_path, table[0], memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime) for table in tables]\n",
    "            with multiprocessing.Pool(processes = 26) as pool:\n",
    "                results = pool.starmap(query, inputs)\n",
    "            # get the pair with the highest ratio\n",
    "            result = max(results, key = lambda pair: pair[1])\n",
    "            n_gram = result[0][0]\n",
    "            f1.write(f\"{n_gram}\\n\")\n",
    "            ratio = result[1]\n",
    "            m_i = result[0][2]\n",
    "            cost = (matches_squared + m_i ** 2) / (total_matches + m_i) - total_avg_runtime\n",
    "            remaining_memory = memory_budget - total_matches\n",
    "            remaining_runtime = runtime_budget - total_avg_runtime\n",
    "            print(remaining_memory)\n",
    "            print(remaining_runtime)\n",
    "            writer.writerow([result[0][1], ratio, m_i, cost, remaining_memory, remaining_runtime])\n",
    "            total_matches += m_i\n",
    "            matches_squared += m_i**2\n",
    "            total_avg_runtime += matches_squared / total_matches  \n",
    "            combination = result[0][0]\n",
    "            with closing(conn.cursor()) as c:\n",
    "                c.execute(f\"UPDATE '{combination}' SET used = 1 WHERE combination = '{combination}' and n_gram = '{n_gram}'\")\n",
    "            print(f\"finished in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/data1/kji/databases/test.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_budget = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_budget = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No multithreading takes 166 seconds per fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lprun` not found.\n"
     ]
    }
   ],
   "source": [
    "%lprun -f construct_db construct_db(db_path, memory_budget, runtime_budget, \"experiments/db_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8000\n",
      "finished in 49.985451221466064 seconds\n",
      "9999\n",
      "7999.0\n",
      "finished in 46.40796089172363 seconds\n",
      "9998\n",
      "7998.0\n",
      "finished in 44.28696370124817 seconds\n",
      "9997\n",
      "7997.0\n",
      "finished in 46.907904386520386 seconds\n",
      "9996\n",
      "7996.0\n",
      "finished in 46.40547823905945 seconds\n",
      "9995\n",
      "7995.0\n",
      "finished in 52.72178411483765 seconds\n",
      "9994\n",
      "7994.0\n",
      "finished in 45.79409456253052 seconds\n",
      "9993\n",
      "7993.0\n",
      "finished in 46.2898633480072 seconds\n",
      "9992\n",
      "7992.0\n",
      "finished in 46.557454109191895 seconds\n",
      "9991\n",
      "7991.0\n",
      "finished in 44.22396373748779 seconds\n",
      "9990\n",
      "7990.0\n",
      "finished in 49.21182632446289 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-311:\n",
      "Process ForkPoolWorker-308:\n",
      "Process ForkPoolWorker-297:\n",
      "Process ForkPoolWorker-293:\n",
      "Process ForkPoolWorker-310:\n",
      "Process ForkPoolWorker-309:\n",
      "Process ForkPoolWorker-304:\n",
      "Process ForkPoolWorker-291:\n",
      "Process ForkPoolWorker-305:\n",
      "Process ForkPoolWorker-296:\n",
      "Process ForkPoolWorker-307:\n",
      "Process ForkPoolWorker-312:\n",
      "Process ForkPoolWorker-306:\n",
      "Process ForkPoolWorker-300:\n",
      "Process ForkPoolWorker-298:\n",
      "Process ForkPoolWorker-294:\n",
      "Process ForkPoolWorker-299:\n",
      "Process ForkPoolWorker-301:\n",
      "Process ForkPoolWorker-303:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89e8c6af3d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconstruct_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experiments/db_tests\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9bb8e07a2a86>\u001b[0m in \u001b[0;36mconstruct_db\u001b[0;34m(db_path, memory_budget, runtime_budget, outdir)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_avg_runtime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m# get the pair with the highest ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "construct_db(db_path, memory_budget, runtime_budget, \"experiments/db_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
