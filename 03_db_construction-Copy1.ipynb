{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import numba as nb\n",
    "import pickle\n",
    "import shelve\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import csv\n",
    "import os\n",
    "import sqlite3\n",
    "import time\n",
    "import dill\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/probabilities.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in iglob(f\"{db_dir}/*_counts.pkl\", recursive=True):\n",
    "    files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.sort(key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesplit1 = [\n",
    "     '/data1/kji/databases/0_counts.pkl',\n",
    "     '/data1/kji/databases/04_counts.pkl',\n",
    "     '/data1/kji/databases/05_counts.pkl',\n",
    "     '/data1/kji/databases/034_counts.pkl',\n",
    "     '/data1/kji/databases/024_counts.pkl',\n",
    "     '/data1/kji/databases/015_counts.pkl',\n",
    "     '/data1/kji/databases/035_counts.pkl',\n",
    "     '/data1/kji/databases/023_counts.pkl',\n",
    "     '/data1/kji/databases/0124_counts.pkl',\n",
    "     '/data1/kji/databases/0135_counts.pkl',\n",
    "     '/data1/kji/databases/0145_counts.pkl',\n",
    "     '/data1/kji/databases/0134_counts.pkl',\n",
    "     '/data1/kji/databases/0234_counts.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesplit2 = [\n",
    "     '/data1/kji/databases/03_counts.pkl',\n",
    "     '/data1/kji/databases/01_counts.pkl',\n",
    "     '/data1/kji/databases/02_counts.pkl',\n",
    "     '/data1/kji/databases/013_counts.pkl',\n",
    "     '/data1/kji/databases/012_counts.pkl',\n",
    "     '/data1/kji/databases/014_counts.pkl',\n",
    "     '/data1/kji/databases/045_counts.pkl',\n",
    "     '/data1/kji/databases/025_counts.pkl',\n",
    "     '/data1/kji/databases/0235_counts.pkl',\n",
    "     '/data1/kji/databases/0123_counts.pkl',\n",
    "     '/data1/kji/databases/0345_counts.pkl',\n",
    "     '/data1/kji/databases/0125_counts.pkl',\n",
    "     '/data1/kji/databases/0245_counts.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_DB(db, combination, outdir):\n",
    "    with open(f\"{outdir}/{combination}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(db, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test database with in memory dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store each n-gram, utility, and the number of matches for an n-gram in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays(file_list):\n",
    "    n_grams = np.array([], dtype = object)\n",
    "    probabilities = np.array([], dtype = float)\n",
    "    matches = np.array([], dtype = int)\n",
    "    start = 0\n",
    "    for filename in file_list:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            d = pickle.load(f)\n",
    "            combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "            length = len(d)            \n",
    "            fps = np.empty(length, dtype = object)\n",
    "            utils = np.full(length, utilities[combination])\n",
    "            values = np.empty(length, dtype = int)\n",
    "            for i, (n_gram, count) in enumerate(d.items()):\n",
    "                fps[i] = n_gram\n",
    "                values[i] = count\n",
    "            d.clear()\n",
    "            n_grams = np.concatenate([n_grams, fps])\n",
    "            probabilities = np.concatenate([probabilities, utils])\n",
    "            matches = np.concatenate([matches, values])\n",
    "            print(f\"finished processing {filename}\")\n",
    "            f.flush()\n",
    "    return n_grams, probabilities, matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark indices as boundaries for each type of n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1 = {}\n",
    "start = 0\n",
    "for filename in filesplit1:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "    length = len(d)\n",
    "    # update indices for the given combination\n",
    "    indices1[combination] = [start, start + length]\n",
    "    start = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices2 = {}\n",
    "start = 0\n",
    "for filename in filesplit2:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "    combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "    length = len(d)\n",
    "    # update indices for the given combination\n",
    "    indices2[combination] = [start, start + length]\n",
    "    start = start + length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1.update(indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(indices1, \"db_indices\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing /data1/kji/databases/03_counts.pkl\n",
      "finished processing /data1/kji/databases/01_counts.pkl\n",
      "finished processing /data1/kji/databases/02_counts.pkl\n",
      "finished processing /data1/kji/databases/013_counts.pkl\n",
      "finished processing /data1/kji/databases/012_counts.pkl\n",
      "finished processing /data1/kji/databases/014_counts.pkl\n",
      "finished processing /data1/kji/databases/045_counts.pkl\n",
      "finished processing /data1/kji/databases/025_counts.pkl\n",
      "finished processing /data1/kji/databases/0235_counts.pkl\n",
      "finished processing /data1/kji/databases/0123_counts.pkl\n",
      "finished processing /data1/kji/databases/0345_counts.pkl\n",
      "finished processing /data1/kji/databases/0125_counts.pkl\n",
      "finished processing /data1/kji/databases/0245_counts.pkl\n"
     ]
    }
   ],
   "source": [
    "n_grams, probs, matches = generate_arrays(filesplit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(n_grams, \"fps_split1\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(probs, \"utils_split1\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(matches, \"matches_split1\", db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate database construction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def parallel_runtime(x, Dm_squared, Dm, Dt, Bt):\n",
    "    y=np.empty(x.shape)\n",
    "    for i in nb.prange(len(x)):\n",
    "        y[i]=((x[i]*x[i] + Dm_squared) / (x[i] + Dm) - Dt) / (Bt - Dt)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def parallel_memory(x, Dm, Bm):\n",
    "    y=np.empty(x.shape)\n",
    "    for i in nb.prange(len(x)):\n",
    "        y[i]=x[i] / (Bm - Dm)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def parallel_max(x, y, utils):\n",
    "    z=np.empty(x.shape)\n",
    "    for i in nb.prange(len(x)):\n",
    "        z[i] = utils[i] / max(x[i], y[i])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 5):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 6), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(Bm, Bt, outdir, outfile_name, k):\n",
    "    \"\"\"\n",
    "    Inputs: a memory budget and a runtime budget, and a number k representing the top k fingerprints we want each time\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    # total number of matches for all fingerprints in the database\n",
    "    Dm = 0\n",
    "    Dm_squared = 0\n",
    "    # cumulative average runtime cost of all fingerprints in the database\n",
    "    Dt = 0\n",
    "    \n",
    "    masks = [None, None]\n",
    "    with open(f\"{outdir}/{outfile_name}.csv\", \"w\") as f2:\n",
    "        writer = csv.writer(f2)\n",
    "        writer.writerow([\"ratio\", \"m_i\", \"Ct_i\", \"Bm-Dm\", \"Bt-Dt\"])\n",
    "        while Bm - Dm > 0 and Bt - Dt > 0:\n",
    "            start = time.time()\n",
    "            candidate_n_grams = np.array([], dtype = object)\n",
    "            candidate_ratios = np.array([], dtype = float)\n",
    "            candidate_indices = np.array([], dtype = int)\n",
    "            candidate_matches = np.array([], dtype = int)\n",
    "            # consider top k from both file splits\n",
    "            for i in range(2):\n",
    "                with open(f'/data1/kji/databases/utils_split{i}.pkl', \"rb\") as f2:\n",
    "                    probabilities = pickle.load(f2)\n",
    "                with open(f'/data1/kji/databases/matches_split{i}.pkl', \"rb\") as f3:\n",
    "                    matches = pickle.load(f3)\n",
    "                    \n",
    "                if masks[i] is None:\n",
    "                    masks[i] = np.zeros(matches.shape)\n",
    "                    print(matches.shape)\n",
    "                else:\n",
    "                    matches[np.nonzero(masks[i])[0]] = 10 ** 9\n",
    "                    \n",
    "                # parallelize computation of marginal costs\n",
    "                marginal_memory = parallel_memory(matches, Dm, Bm)\n",
    "                marginal_runtime = parallel_runtime(matches, Dm_squared, Dm, Dt, Bt)\n",
    "                ratios = parallel_max(marginal_memory, marginal_runtime, probabilities)\n",
    "                del probabilities\n",
    "                \n",
    "                # get the top k fingerprints with the highest utility:cost ratio\n",
    "                idx = np.argpartition(ratios, -k)[-k:]\n",
    "                candidate_ratios = np.concatenate((candidate_ratios, ratios[idx]))\n",
    "                del ratios\n",
    "                candidate_matches = np.concatenate((candidate_matches, matches[idx]))\n",
    "                del matches\n",
    "                \n",
    "                with open(f\"/data1/kji/databases/fps_split{i}.pkl\", \"rb\") as f1:\n",
    "                    n_grams = pickle.load(f1)\n",
    "                candidate_n_grams = np.concatenate((candidate_n_grams, n_grams[idx]))\n",
    "                del n_grams\n",
    "                candidate_indices = np.concatenate((candidate_indices, idx))\n",
    "                del idx\n",
    "                \n",
    "                assert candidate_ratios.shape == candidate_matches.shape == candidate_n_grams.shape == candidate_indices.shape\n",
    "            # after looping over both, we have length 2*k arrays storing all the information we need        \n",
    "            final_idx = np.argpartition(candidate_ratios, -k)[-k:]\n",
    "            final_matches = candidate_matches[final_idx]\n",
    "            first_half = final_idx < k\n",
    "            idx_split1 = candidate_indices[final_idx[first_half]]\n",
    "            idx_split2 = candidate_indices[final_idx[~first_half]]\n",
    "            \n",
    "            # use mask arrays to keep track of fingerprints we've selected\n",
    "            if idx_split1.size:\n",
    "                masks[0][idx_split1] = 1\n",
    "            if idx_split2.size:\n",
    "                masks[1][idx_split2] = 1\n",
    "                \n",
    "            Dm += np.sum(final_matches)\n",
    "            Dm_squared += np.sum(final_matches**2)\n",
    "            cost = Dm_squared / Dm - Dt\n",
    "            Dt = Dm_squared / Dm\n",
    "            remaining_memory = Bm - Dm\n",
    "            remaining_runtime = Bt - Dt\n",
    "            writer.writerow([candidate_ratios[final_idx[-1]], final_matches[-1], cost, remaining_memory, remaining_runtime])\n",
    "\n",
    "            # free memory\n",
    "            candidate_n_grams = None\n",
    "            candidate_ratios = None\n",
    "            candidate_indices = None\n",
    "            candidate_matches = None\n",
    "            \n",
    "    # write all used fingerprints to a file\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as out:\n",
    "        with open(f'/data1/kji/databases/db_indices.pkl', \"rb\") as f:\n",
    "            db_indices = pickle.load(f)\n",
    "        for i in range(2):\n",
    "            with open(f\"/data1/kji/databases/fps_split{i}.pkl\", \"rb\") as f:\n",
    "                n_grams = pickle.load(f)\n",
    "                used = np.nonzero(masks[i])[0]\n",
    "                masks[i] = None\n",
    "                for j in used:\n",
    "                    for combination in combinations:\n",
    "                        start, end = db_indices[combination]\n",
    "                        if start <= j < end:\n",
    "                            out.write(f\"{n_grams[j]} {combination}\\n\")\n",
    "                            break\n",
    "                del n_grams\n",
    "                del used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bm = 200000000\n",
    "Bt = 200000000\n",
    "k = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329995024,)\n",
      "(336214600,)\n"
     ]
    }
   ],
   "source": [
    "construct_db(Bm, Bt, \"/data1/kji/db_tests\", \"200mill\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct database of offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected all the fingerprints, we construct the database containing each fingerprint and their offsets in IMSLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_file =  \"/data1/kji/db_tests/test.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a dictionary with all the n-grams in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_entry(line):\n",
    "    line = line.rstrip().split()\n",
    "    n_gram, combination = ''.join(line[:-1]), line[-1]\n",
    "    return ast.literal_eval(n_gram), combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db(fp_file):\n",
    "    with open(fp_file) as f:\n",
    "        lines = f.readlines()\n",
    "    n_cores = 30\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    keys = pool.map(initialize_entry, lines)\n",
    "    dbs = {combination: {} for combination in combinations}\n",
    "    for fp, combination in keys:\n",
    "        dbs[combination][fp] = {}\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = make_db(fp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in combinations:\n",
    "    with open(f\"/data1/kji/databases/sub_dbs/{combination}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dbs[combination], f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/135mill_empty.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dbs, f, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in every single database file and update our current database with the real offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in iglob(f\"{db_dir}/*.pkl\", recursive=True):\n",
    "    files.append(filename)\n",
    "files.sort(key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data1/kji/databases/0.pkl',\n",
       " '/data1/kji/databases/02.pkl',\n",
       " '/data1/kji/databases/03.pkl',\n",
       " '/data1/kji/databases/05.pkl',\n",
       " '/data1/kji/databases/04.pkl',\n",
       " '/data1/kji/databases/01.pkl',\n",
       " '/data1/kji/databases/015.pkl',\n",
       " '/data1/kji/databases/014.pkl',\n",
       " '/data1/kji/databases/035.pkl',\n",
       " '/data1/kji/databases/013.pkl',\n",
       " '/data1/kji/databases/023.pkl',\n",
       " '/data1/kji/databases/025.pkl',\n",
       " '/data1/kji/databases/045.pkl',\n",
       " '/data1/kji/databases/034.pkl',\n",
       " '/data1/kji/databases/012.pkl',\n",
       " '/data1/kji/databases/024.pkl',\n",
       " '/data1/kji/databases/0125.pkl',\n",
       " '/data1/kji/databases/0124.pkl',\n",
       " '/data1/kji/databases/0134.pkl',\n",
       " '/data1/kji/databases/0135.pkl',\n",
       " '/data1/kji/databases/0235.pkl',\n",
       " '/data1/kji/databases/0345.pkl',\n",
       " '/data1/kji/databases/0123.pkl',\n",
       " '/data1/kji/databases/0145.pkl',\n",
       " '/data1/kji/databases/0234.pkl',\n",
       " '/data1/kji/databases/0245.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_db_values(file, db):\n",
    "    with open(file, \"rb\") as f:\n",
    "        d = dill.load(f)\n",
    "    for n_gram in d.keys():\n",
    "        if n_gram in db:\n",
    "            if db[n_gram] is None:\n",
    "                db[n_gram] = d[n_gram]\n",
    "            else:\n",
    "                db[n_gram].update(d[n_gram])\n",
    "    d.clear()\n",
    "    print(f\"finished {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished /data1/kji/databases/0.pkl\n",
      "finished /data1/kji/databases/01.pkl\n",
      "finished /data1/kji/databases/02.pkl\n",
      "finished /data1/kji/databases/03.pkl\n",
      "finished /data1/kji/databases/04.pkl\n",
      "finished /data1/kji/databases/05.pkl\n",
      "finished /data1/kji/databases/015.pkl\n",
      "finished /data1/kji/databases/024.pkl\n",
      "finished /data1/kji/databases/034.pkl\n",
      "finished /data1/kji/databases/035.pkl\n"
     ]
    }
   ],
   "source": [
    "for combination in combinations:\n",
    "    if dbs[combination]:\n",
    "        add_db_values(f\"{db_dir}/{combination}.pkl\", dbs[combination])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/split21_Bt_200mill.pkl\", \"wb\") as f:\n",
    "    pickle.dump(database, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf26b664e015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mshelve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data1/kji/databases2/shelve_test\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0madd_db_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDbfilenameShelf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/shelve.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, flag, protocol, writeback)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mShelf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriteback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/dbm/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/dbm/dumb.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(file, flag, mode)\u001b[0m\n\u001b[1;32m    322\u001b[0m         warnings.warn(\"Flag must be one of 'r', 'w', 'c', or 'n'\",\n\u001b[1;32m    323\u001b[0m                       DeprecationWarning, stacklevel=2)\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_Database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/dbm/dumb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filebasename, mode, flag)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Handle the creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/dbm/dumb.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_and_siz_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Latin-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_and_siz_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with shelve.open(\"/data1/kji/databases2/shelve_test\") as db:\n",
    "    add_db_values(files[:16], db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished /data1/kji/databases2/034.pkl\n",
      "finished /data1/kji/databases2/012.pkl\n",
      "finished /data1/kji/databases2/024.pkl\n"
     ]
    }
   ],
   "source": [
    "add_db_values(files[13:16], database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/split2_Bt_200mill.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for file in test_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        d = dill.load(f)\n",
    "        combined.update(d)\n",
    "        d.clear()\n",
    "print(f\"finished in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fp in database:\n",
    "    database[fp] = combined[fp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases2/140mill.pkl\", \"w\") as f:\n",
    "    pickle.dump(database, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('sqlite:////data1/kji/databases/test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['/data1/kji/databases/0_counts.pkl', \n",
    "              '/data1/kji/databases/01_counts.pkl',\n",
    "              '/data1/kji/databases/012_counts.pkl',\n",
    "              '/data1/kji/databases/0123_counts.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing /data1/kji/databases/0_counts.pkl\n",
      "finished processing /data1/kji/databases/01_counts.pkl\n",
      "finished processing /data1/kji/databases/012_counts.pkl\n",
      "finished processing /data1/kji/databases/0123_counts.pkl\n"
     ]
    }
   ],
   "source": [
    "for filename in test_files:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        df_dict = defaultdict(list)\n",
    "        combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "        for n_gram, count in d.items():\n",
    "            df_dict['combination'].append(combination)\n",
    "            df_dict['n_gram'].append(str(n_gram))\n",
    "            df_dict['matches'].append(count)\n",
    "            df_dict['utility'].append(utilities[combination])\n",
    "            df_dict['used'].append(0)\n",
    "        d.clear()\n",
    "        df = pd.DataFrame.from_dict(df_dict)\n",
    "        with engine.begin() as connection:\n",
    "            df.to_sql(combination, con=connection, index=False, if_exists='replace')\n",
    "        print(f\"finished processing {filename}\")\n",
    "        f.flush()\n",
    "        df_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate database construction plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_memory(num_matches, remaining_budget):\n",
    "    return num_matches / remaining_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget):\n",
    "    cost = (matches_squared + num_matches ** 2) / (total_matches + num_matches) - total_avg_runtime\n",
    "    return cost / (runtime_budget - total_avg_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(utility, num_matches, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    marginal_memory_cost = marginal_memory(num_matches, memory_budget-total_matches)\n",
    "    marginal_runtime_cost = marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget)\n",
    "    return utility / max(marginal_memory_cost, marginal_runtime_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/data1/kji/databases/test.db\")\n",
    "conn.create_function(\"metric\", 7, metric)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_items = ['(2048, 0, 0, 0, 0, 0)', '(1073741824, 0, 0, 0, 0, 0)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from fingerprints where used == 0 order by utility desc limit 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = c.execute(query).fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db_path, table_name, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    with closing(sqlite3.connect(db_path)) as con, con,  \\\n",
    "            closing(con.cursor()) as cur:\n",
    "        con.create_function(\"metric\", 7, metric)\n",
    "        query = f\"select * from '{table_name}' where used = 0 order by metric(utility, matches, ?, ?, ?, ?, ?) desc limit 1\"\n",
    "        cur.execute(query, (memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime))\n",
    "        result = cur.fetchone()\n",
    "        ratio = metric(result[3], result[2], memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime)\n",
    "        return (result, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(db_path, memory_budget, runtime_budget, outdir):\n",
    "    \"\"\"\n",
    "    Inputs: a database which contains n-grams, their utility, and number of matches,\n",
    "            a memory budget, and a runtime budget\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    with closing(conn.cursor()) as c:\n",
    "        c.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = c.fetchall()\n",
    "        \n",
    "    # clear all used bits in the database\n",
    "    for table in tables:\n",
    "        with closing(conn.cursor()) as cur:\n",
    "            cur.execute(f\"UPDATE '{table[0]}' SET used = 0\")\n",
    "    \n",
    "    total_matches = 0\n",
    "    matches_squared = 0\n",
    "    total_avg_runtime = 0\n",
    "    with open(f\"{outdir}/Bm_10k_Bt_8k_fingerprints.txt\", \"w\") as f1, open(f\"{outdir}/Bm_10k_Bt_8k_info.csv\", \"w\") as f2:\n",
    "        writer = csv.writer(f2)\n",
    "        writer.writerow([\"fingerprint\", \"ratio\", \"m_i\", \"Ct_i\", \"Bm-Dm\", \"Bt-Dt\"])\n",
    "        while memory_budget - total_matches > 0 and runtime_budget - total_avg_runtime > 0:\n",
    "            start = time.time()\n",
    "            inputs = [(db_path, table[0], memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime) for table in tables]\n",
    "            with multiprocessing.Pool(processes = 26) as pool:\n",
    "                results = pool.starmap(query, inputs)\n",
    "            # get the pair with the highest ratio\n",
    "            result = max(results, key = lambda pair: pair[1])\n",
    "            n_gram = result[0][0]\n",
    "            f1.write(f\"{n_gram}\\n\")\n",
    "            ratio = result[1]\n",
    "            m_i = result[0][2]\n",
    "            cost = (matches_squared + m_i ** 2) / (total_matches + m_i) - total_avg_runtime\n",
    "            remaining_memory = memory_budget - total_matches\n",
    "            remaining_runtime = runtime_budget - total_avg_runtime\n",
    "            print(remaining_memory)\n",
    "            print(remaining_runtime)\n",
    "            writer.writerow([result[0][1], ratio, m_i, cost, remaining_memory, remaining_runtime])\n",
    "            total_matches += m_i\n",
    "            matches_squared += m_i**2\n",
    "            total_avg_runtime += matches_squared / total_matches  \n",
    "            combination = result[0][0]\n",
    "            with closing(conn.cursor()) as c:\n",
    "                c.execute(f\"UPDATE '{combination}' SET used = 1 WHERE combination = '{combination}' and n_gram = '{n_gram}'\")\n",
    "            print(f\"finished in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/data1/kji/databases/test.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_budget = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_budget = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No multithreading takes 166 seconds per fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lprun` not found.\n"
     ]
    }
   ],
   "source": [
    "%lprun -f construct_db construct_db(db_path, memory_budget, runtime_budget, \"experiments/db_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8000\n",
      "finished in 49.985451221466064 seconds\n",
      "9999\n",
      "7999.0\n",
      "finished in 46.40796089172363 seconds\n",
      "9998\n",
      "7998.0\n",
      "finished in 44.28696370124817 seconds\n",
      "9997\n",
      "7997.0\n",
      "finished in 46.907904386520386 seconds\n",
      "9996\n",
      "7996.0\n",
      "finished in 46.40547823905945 seconds\n",
      "9995\n",
      "7995.0\n",
      "finished in 52.72178411483765 seconds\n",
      "9994\n",
      "7994.0\n",
      "finished in 45.79409456253052 seconds\n",
      "9993\n",
      "7993.0\n",
      "finished in 46.2898633480072 seconds\n",
      "9992\n",
      "7992.0\n",
      "finished in 46.557454109191895 seconds\n",
      "9991\n",
      "7991.0\n",
      "finished in 44.22396373748779 seconds\n",
      "9990\n",
      "7990.0\n",
      "finished in 49.21182632446289 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-311:\n",
      "Process ForkPoolWorker-308:\n",
      "Process ForkPoolWorker-297:\n",
      "Process ForkPoolWorker-293:\n",
      "Process ForkPoolWorker-310:\n",
      "Process ForkPoolWorker-309:\n",
      "Process ForkPoolWorker-304:\n",
      "Process ForkPoolWorker-291:\n",
      "Process ForkPoolWorker-305:\n",
      "Process ForkPoolWorker-296:\n",
      "Process ForkPoolWorker-307:\n",
      "Process ForkPoolWorker-312:\n",
      "Process ForkPoolWorker-306:\n",
      "Process ForkPoolWorker-300:\n",
      "Process ForkPoolWorker-298:\n",
      "Process ForkPoolWorker-294:\n",
      "Process ForkPoolWorker-299:\n",
      "Process ForkPoolWorker-301:\n",
      "Process ForkPoolWorker-303:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89e8c6af3d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconstruct_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experiments/db_tests\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9bb8e07a2a86>\u001b[0m in \u001b[0;36mconstruct_db\u001b[0;34m(db_path, memory_budget, runtime_budget, outdir)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_avg_runtime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m# get the pair with the highest ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "construct_db(db_path, memory_budget, runtime_budget, \"experiments/db_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
