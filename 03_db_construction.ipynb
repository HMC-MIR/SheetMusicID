{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import numba as nb\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import dill\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/probabilities.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 6), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_DB(db, combination, outdir):\n",
    "    with open(f\"{outdir}/{combination}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(db, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test database with in memory dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store each n-gram, utility, and the number of matches for an n-gram in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(combinations):\n",
    "    mapping[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pdfs = 30275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays(fp_matches):\n",
    "    n_grams = np.array([], dtype = object)\n",
    "    probabilities = np.array([], dtype = float)\n",
    "    matches = np.array([], dtype = int)\n",
    "    types = np.array([], dtype = int)\n",
    "    for combination in fp_matches:\n",
    "        d = fp_matches[combination]\n",
    "        length = len(d)       \n",
    "        # update indices for the given combination\n",
    "        fps = np.empty(length, dtype = object)\n",
    "        utils = np.empty(length, dtype = float)\n",
    "        values = np.empty(length, dtype = int)\n",
    "        cur_types = np.full(length, mapping[combination])\n",
    "        for i, (n_gram, (count, num_pdfs)) in enumerate(d.items()):\n",
    "            fps[i] = n_gram\n",
    "            values[i] = count\n",
    "#             utils[i] = -utilities[combination] * np.log2(num_pdfs / total_pdfs)\n",
    "#             utils[i] = utilities[combination] * count\n",
    "            utils[i] = utilities[combination] * num_pdfs\n",
    "        d.clear()\n",
    "        n_grams = np.concatenate([n_grams, fps])\n",
    "        probabilities = np.concatenate([probabilities, utils])\n",
    "        matches = np.concatenate([matches, values])\n",
    "        types = np.concatenate([types, cur_types])\n",
    "        print(f\"finished processing {combination}\")\n",
    "    return n_grams, probabilities, matches, types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark indices as boundaries for each type of n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_matches_file = f\"{db_dir}/fp_matches.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fp_matches_file, \"rb\") as f:\n",
    "    fp_matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing 0\n",
      "finished processing 01\n",
      "finished processing 02\n",
      "finished processing 03\n",
      "finished processing 04\n",
      "finished processing 05\n",
      "finished processing 012\n",
      "finished processing 013\n",
      "finished processing 014\n",
      "finished processing 015\n",
      "finished processing 023\n",
      "finished processing 024\n",
      "finished processing 025\n",
      "finished processing 034\n",
      "finished processing 035\n",
      "finished processing 045\n"
     ]
    }
   ],
   "source": [
    "n_grams, probs, matches, types = generate_arrays(fp_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v4_pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(n_grams, \"fps\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(probs, \"utils\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(matches, \"matches\", db_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(types, \"n_gram_types\", db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate database construction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for n_gram in range(1, 4):\n",
    "    combinations += [[0] + list(tup) for tup in itertools.combinations(range(1, 6), n_gram-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\"\".join(str(num) for num in combination) for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_bscore_length = 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db_thresholded(n_grams, utilities, types, matches, threshold, outdir, outfile_name):\n",
    "    \"\"\"\n",
    "    Input: an array of n-grams, an array of each n-gram's utility, an array of the number of matches\n",
    "           for each n-gram in IMSLP, and an array of each n-gram's type (e.g. '012')\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database that have at most threshold matches\n",
    "    \"\"\"\n",
    "    # sort fingerprints in descending order\n",
    "    idx = np.argsort(-utilities)\n",
    "            \n",
    "    # write all used fingerprints to a file\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as out:\n",
    "        for i in idx:\n",
    "            if matches[i] > threshold:\n",
    "                continue\n",
    "            out.write(f\"{n_grams[i]} {types[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(n_grams, utilities, types, outdir, outfile_name):\n",
    "    \"\"\"\n",
    "    Input: an array of n-grams, an array of each n-gram's utility, and an array of each n-gram's type (e.g. '012')\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    # sort fingerprints in descending order\n",
    "    idx = np.argsort(utilities)\n",
    "            \n",
    "    # write all used fingerprints to a file\n",
    "    count = 0\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as out:\n",
    "        for i in idx:\n",
    "            if count > 80000000:\n",
    "                break\n",
    "            count += 1\n",
    "            out.write(f\"{n_grams[i]} {types[i]} {-utilities[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(n_grams, utilities, types, outdir, outfile_name):\n",
    "    \"\"\"\n",
    "    Input: an array of n-grams, an array of each n-gram's utility, and an array of each n-gram's type (e.g. '012')\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    # sort fingerprints in descending order\n",
    "    idx = np.argsort(-utilities)\n",
    "            \n",
    "    # write all used fingerprints to a file\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as out:\n",
    "        for i in idx:\n",
    "            out.write(f\"{n_grams[i]} {types[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_v4_pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{db_dir}/fps.pkl\", \"rb\") as f:\n",
    "    n_grams = pickle.load(f)\n",
    "with open(f\"{db_dir}/utils.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)\n",
    "with open(f\"{db_dir}/n_gram_types.pkl\", \"rb\") as f:\n",
    "    types = pickle.load(f)\n",
    "with open(f\"{db_dir}/matches.pkl\", \"rb\") as f:\n",
    "    matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_db_thresholded(n_grams, utilities, types, matches, 10000, \"/data1/kji/construction_lists\", \"all_v4.0.d_pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_db(n_grams, utilities, types, \"/data1/kji\", \"v3_test_10mill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct database of offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected all the fingerprints, we construct the database containing each fingerprint and their offsets in IMSLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_file = \"/data1/kji/construction_lists/103mill_v4.0d_pdfs.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a dictionary with all the n-grams in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(combinations):\n",
    "    reverse_mapping[str(i)] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_entry(line):\n",
    "    line = line.rstrip().split()\n",
    "    n_gram, combination = ''.join(line[:-1]), line[-1]\n",
    "    return ast.literal_eval(n_gram), reverse_mapping[combination]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db(fp_file):\n",
    "    with open(fp_file) as f:\n",
    "        lines = f.readlines()\n",
    "    n_cores = 30\n",
    "    pool = multiprocessing.Pool(n_cores)\n",
    "    keys = pool.map(initialize_entry, lines)\n",
    "    dbs = {combination: {} for combination in combinations}\n",
    "    for fp, combination in keys:\n",
    "        dbs[combination][fp] = {}\n",
    "    return dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbs = make_db(fp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load in every single database file and update our current database with the real offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases_random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_db_values(file, db):\n",
    "    with open(file, \"rb\") as f:\n",
    "        d = dill.load(f)\n",
    "    for n_gram in d.keys():\n",
    "        if n_gram in db:\n",
    "            db[n_gram] = dict(d[n_gram])\n",
    "            total_count = 0\n",
    "            for piece in db[n_gram]:\n",
    "                total_count += len(db[n_gram][piece])\n",
    "                db[n_gram][piece] = tuple(db[n_gram][piece])\n",
    "            db[n_gram] = (total_count, db[n_gram])\n",
    "    d.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in dbs:\n",
    "    if dbs[combination]:\n",
    "        add_db_values(f\"{db_dir}/{combination}.pkl\", dbs[combination])\n",
    "        with open(f\"/data1/kji/databases_v4_pdfs/103mill/{combination}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(dbs[combination], f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        dbs[combination].clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
