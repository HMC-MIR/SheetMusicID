{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "import numpy.ma as ma\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import multiprocessing\n",
    "import csv\n",
    "import os\n",
    "import sqlite3\n",
    "import time\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/kji/databases/probabilities.pkl\", \"rb\") as f:\n",
    "    utilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dictionary of n-grams and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/data1/kji/databases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for filename in iglob(f\"{db_dir}/*_counts.pkl\", recursive=True):\n",
    "    files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.sort(key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_memory(num_matches, remaining_budget):\n",
    "    return num_matches / remaining_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget):\n",
    "    cost = (matches_squared + num_matches ** 2) / (total_matches + num_matches) - total_avg_runtime\n",
    "    return cost / (runtime_budget - total_avg_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(value_pair, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    print(value_pair)\n",
    "    utility = value_pair[0]\n",
    "    num_matches = value_pair[1]\n",
    "    marginal_memory_cost = marginal_memory(num_matches, memory_budget-total_matches)\n",
    "    marginal_runtime_cost = marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget)\n",
    "    return utility / max(marginal_memory_cost, marginal_runtime_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_DB(db, combination, outdir):\n",
    "    with open(f\"{outdir}/{combination}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(db, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test database with in memory dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First work with a subset of the data for prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['/data1/kji/databases/0_counts.pkl', \n",
    "              '/data1/kji/databases/01_counts.pkl',\n",
    "              '/data1/kji/databases/012_counts.pkl',\n",
    "              '/data1/kji/databases/0123_counts.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store each n-gram, utility, and the number of matches for an n-gram in a separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = np.array([], dtype = object)\n",
    "probabilities = np.array([], dtype = float)\n",
    "matches = np.array([], dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing /data1/kji/databases/0_counts.pkl\n",
      "finished processing /data1/kji/databases/01_counts.pkl\n",
      "finished processing /data1/kji/databases/012_counts.pkl\n",
      "finished processing /data1/kji/databases/0123_counts.pkl\n"
     ]
    }
   ],
   "source": [
    "for filename in test_files:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "        length = len(d)\n",
    "        fps = np.empty(length, dtype = object)\n",
    "        utils = np.full(length, utilities[combination])\n",
    "        values = np.empty(length, dtype = int)\n",
    "        for i, (n_gram, count) in enumerate(d.items()):\n",
    "            fps[i] = n_gram\n",
    "            values[i] = count\n",
    "        d.clear()\n",
    "        n_grams = np.concatenate([n_grams, fps])\n",
    "        probabilities = np.concatenate([probabilities, utils])\n",
    "        matches = np.concatenate([matches, values])\n",
    "        print(f\"finished processing {filename}\")\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(n_grams, \"fps_1-4\", '/data1/kji/databases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(probabilities, \"utils_1-4\", '/data1/kji/databases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_DB(matches, \"matches_1-4\", '/data1/kji/databases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data1/kji/databases/fps_1-4/pkl', \"rb\") as f:\n",
    "    n_grams = pickle.load(f)\n",
    "    \n",
    "with open('/data1/kji/databases/utils_1-4/pkl', \"rb\") as f:\n",
    "    probabilities = pickle.load(f)\n",
    "    \n",
    "with open('/data1/kji/databases/matches_1-4/pkl', \"rb\") as f:\n",
    "    matches = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(Bm, Bt, outdir, outfile_name):\n",
    "    \"\"\"\n",
    "    Inputs: a memory budget and a runtime budget\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    # total number of matches for all fingerprints in the database\n",
    "    Dm = 0\n",
    "    Dm_squared = 0\n",
    "    # cumulative average runtime cost of all fingerprints in the database\n",
    "    Dt = 0\n",
    "    mask = np.zeros(n_grams.shape)\n",
    "    with open(f\"{outdir}/{outfile_name}.txt\", \"w\") as f1, open(f\"{outdir}/{outfile_name}.csv\", \"w\") as f2:\n",
    "        writer = csv.writer(f2)\n",
    "        writer.writerow([\"fingerprint\", \"ratio\", \"m_i\", \"Ct_i\", \"Bm-Dm\", \"Bt-Dt\"])\n",
    "        while Bm - Dm > 0 and Bt - Dt > 0:\n",
    "            start = time.time()\n",
    "            f = lambda x: x / (Bm-Dm)\n",
    "            g = lambda x: ((x ** 2 + Dm_squared) / (x + Dm) - Dt) / (Bt - Dt)\n",
    "            # vectorize computation of marginal costs\n",
    "            marginal_memory = f(matches)\n",
    "            marginal_runtime = g(matches)\n",
    "            ratios = probabilities / np.maximum(marginal_memory, marginal_runtime)\n",
    "            # use a mask array to keep track of which fingerprints we've already selected\n",
    "            x = ma.array(ratios, mask = mask)\n",
    "            # get the fingerprint with the highest utility:cost ratio\n",
    "            idx = np.argmax(x)\n",
    "            result = n_grams[idx]\n",
    "            mask[idx] = 1\n",
    "            f1.write(f\"{result}\\n\")\n",
    "            \n",
    "            ratio = ratios[idx]\n",
    "            m_i = matches[idx]\n",
    "            \n",
    "            Dm += m_i\n",
    "            Dm_squared += m_i**2\n",
    "            \n",
    "            cost = Dm_squared / Dm - Dt\n",
    "            Dt = Dm_squared / Dm\n",
    "            \n",
    "            remaining_memory = Bm - Dm\n",
    "            remaining_runtime = Bt - Dt\n",
    "            writer.writerow([result, ratio, m_i, cost, remaining_memory, remaining_runtime])\n",
    "#             print(f\"finished in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_budget = 10000\n",
    "runtime_budget = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_db(memory_budget, runtime_budget, \"experiments/db_tests\", \"numpy_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_db(100000, 100000, \"experiments/db_tests\", \"numpy_test_100k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('sqlite:////data1/kji/databases/test.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = ['/data1/kji/databases/0_counts.pkl', \n",
    "              '/data1/kji/databases/01_counts.pkl',\n",
    "              '/data1/kji/databases/012_counts.pkl',\n",
    "              '/data1/kji/databases/0123_counts.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing /data1/kji/databases/0_counts.pkl\n",
      "finished processing /data1/kji/databases/01_counts.pkl\n",
      "finished processing /data1/kji/databases/012_counts.pkl\n",
      "finished processing /data1/kji/databases/0123_counts.pkl\n"
     ]
    }
   ],
   "source": [
    "for filename in test_files:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        df_dict = defaultdict(list)\n",
    "        combination = filename.split('/')[-1].split(\"_\")[0]\n",
    "        for n_gram, count in d.items():\n",
    "            df_dict['combination'].append(combination)\n",
    "            df_dict['n_gram'].append(str(n_gram))\n",
    "            df_dict['matches'].append(count)\n",
    "            df_dict['utility'].append(utilities[combination])\n",
    "            df_dict['used'].append(0)\n",
    "        d.clear()\n",
    "        df = pd.DataFrame.from_dict(df_dict)\n",
    "        with engine.begin() as connection:\n",
    "            df.to_sql(combination, con=connection, index=False, if_exists='replace')\n",
    "        print(f\"finished processing {filename}\")\n",
    "        f.flush()\n",
    "        df_dict.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate database construction plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_memory(num_matches, remaining_budget):\n",
    "    return num_matches / remaining_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget):\n",
    "    cost = (matches_squared + num_matches ** 2) / (total_matches + num_matches) - total_avg_runtime\n",
    "    return cost / (runtime_budget - total_avg_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(utility, num_matches, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    marginal_memory_cost = marginal_memory(num_matches, memory_budget-total_matches)\n",
    "    marginal_runtime_cost = marginal_runtime(matches_squared, num_matches, total_matches, total_avg_runtime, runtime_budget)\n",
    "    return utility / max(marginal_memory_cost, marginal_runtime_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/data1/kji/databases/test.db\")\n",
    "conn.create_function(\"metric\", 7, metric)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_items = ['(2048, 0, 0, 0, 0, 0)', '(1073741824, 0, 0, 0, 0, 0)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from fingerprints where used == 0 order by utility desc limit 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = c.execute(query).fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db_path, table_name, memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime):\n",
    "    with closing(sqlite3.connect(db_path)) as con, con,  \\\n",
    "            closing(con.cursor()) as cur:\n",
    "        con.create_function(\"metric\", 7, metric)\n",
    "        query = f\"select * from '{table_name}' where used = 0 order by metric(utility, matches, ?, ?, ?, ?, ?) desc limit 1\"\n",
    "        cur.execute(query, (memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime))\n",
    "        result = cur.fetchone()\n",
    "        ratio = metric(result[3], result[2], memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime)\n",
    "        return (result, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_db(db_path, memory_budget, runtime_budget, outdir):\n",
    "    \"\"\"\n",
    "    Inputs: a database which contains n-grams, their utility, and number of matches,\n",
    "            a memory budget, and a runtime budget\n",
    "    Output: a file specifying an ordered list of n-grams to include in the final database\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    with closing(conn.cursor()) as c:\n",
    "        c.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "        tables = c.fetchall()\n",
    "        \n",
    "    # clear all used bits in the database\n",
    "    for table in tables:\n",
    "        with closing(conn.cursor()) as cur:\n",
    "            cur.execute(f\"UPDATE '{table[0]}' SET used = 0\")\n",
    "    \n",
    "    total_matches = 0\n",
    "    matches_squared = 0\n",
    "    total_avg_runtime = 0\n",
    "    with open(f\"{outdir}/Bm_10k_Bt_8k_fingerprints.txt\", \"w\") as f1, open(f\"{outdir}/Bm_10k_Bt_8k_info.csv\", \"w\") as f2:\n",
    "        writer = csv.writer(f2)\n",
    "        writer.writerow([\"fingerprint\", \"ratio\", \"m_i\", \"Ct_i\", \"Bm-Dm\", \"Bt-Dt\"])\n",
    "        while memory_budget - total_matches > 0 and runtime_budget - total_avg_runtime > 0:\n",
    "            start = time.time()\n",
    "            inputs = [(db_path, table[0], memory_budget, runtime_budget, total_matches, matches_squared, total_avg_runtime) for table in tables]\n",
    "            with multiprocessing.Pool(processes = 26) as pool:\n",
    "                results = pool.starmap(query, inputs)\n",
    "            # get the pair with the highest ratio\n",
    "            result = max(results, key = lambda pair: pair[1])\n",
    "            n_gram = result[0][0]\n",
    "            f1.write(f\"{n_gram}\\n\")\n",
    "            ratio = result[1]\n",
    "            m_i = result[0][2]\n",
    "            cost = (matches_squared + m_i ** 2) / (total_matches + m_i) - total_avg_runtime\n",
    "            remaining_memory = memory_budget - total_matches\n",
    "            remaining_runtime = runtime_budget - total_avg_runtime\n",
    "            print(remaining_memory)\n",
    "            print(remaining_runtime)\n",
    "            writer.writerow([result[0][1], ratio, m_i, cost, remaining_memory, remaining_runtime])\n",
    "            total_matches += m_i\n",
    "            matches_squared += m_i**2\n",
    "            total_avg_runtime += matches_squared / total_matches  \n",
    "            combination = result[0][0]\n",
    "            with closing(conn.cursor()) as c:\n",
    "                c.execute(f\"UPDATE '{combination}' SET used = 1 WHERE combination = '{combination}' and n_gram = '{n_gram}'\")\n",
    "            print(f\"finished in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"/data1/kji/databases/test.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_budget = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_budget = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No multithreading takes 166 seconds per fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lprun` not found.\n"
     ]
    }
   ],
   "source": [
    "%lprun -f construct_db construct_db(db_path, memory_budget, runtime_budget, \"experiments/db_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8000\n",
      "finished in 49.985451221466064 seconds\n",
      "9999\n",
      "7999.0\n",
      "finished in 46.40796089172363 seconds\n",
      "9998\n",
      "7998.0\n",
      "finished in 44.28696370124817 seconds\n",
      "9997\n",
      "7997.0\n",
      "finished in 46.907904386520386 seconds\n",
      "9996\n",
      "7996.0\n",
      "finished in 46.40547823905945 seconds\n",
      "9995\n",
      "7995.0\n",
      "finished in 52.72178411483765 seconds\n",
      "9994\n",
      "7994.0\n",
      "finished in 45.79409456253052 seconds\n",
      "9993\n",
      "7993.0\n",
      "finished in 46.2898633480072 seconds\n",
      "9992\n",
      "7992.0\n",
      "finished in 46.557454109191895 seconds\n",
      "9991\n",
      "7991.0\n",
      "finished in 44.22396373748779 seconds\n",
      "9990\n",
      "7990.0\n",
      "finished in 49.21182632446289 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-311:\n",
      "Process ForkPoolWorker-308:\n",
      "Process ForkPoolWorker-297:\n",
      "Process ForkPoolWorker-293:\n",
      "Process ForkPoolWorker-310:\n",
      "Process ForkPoolWorker-309:\n",
      "Process ForkPoolWorker-304:\n",
      "Process ForkPoolWorker-291:\n",
      "Process ForkPoolWorker-305:\n",
      "Process ForkPoolWorker-296:\n",
      "Process ForkPoolWorker-307:\n",
      "Process ForkPoolWorker-312:\n",
      "Process ForkPoolWorker-306:\n",
      "Process ForkPoolWorker-300:\n",
      "Process ForkPoolWorker-298:\n",
      "Process ForkPoolWorker-294:\n",
      "Process ForkPoolWorker-299:\n",
      "Process ForkPoolWorker-301:\n",
      "Process ForkPoolWorker-303:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89e8c6af3d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconstruct_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"experiments/db_tests\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9bb8e07a2a86>\u001b[0m in \u001b[0;36mconstruct_db\u001b[0;34m(db_path, memory_budget, runtime_budget, outdir)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_matches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches_squared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_avg_runtime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m# get the pair with the highest ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SheetMidiSearchRetrieval/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "construct_db(db_path, memory_budget, runtime_budget, \"experiments/db_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
